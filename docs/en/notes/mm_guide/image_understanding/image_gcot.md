---
title: Image Grounded CoT (GCoT) Pipeline
icon: mdi:image-text
createTime: 2026/01/11 20:44:55
permalink: /en/mm_guide/image_gcot/
---
## 1. Overview

The **Image Grounded Chain-of-Thought (GCoT) Pipeline** is designed to automatically generate **Grounded Chain-of-Thought** data. This pipeline generates multi-step reasoning to answer a question and simultaneously spatially locates (via Bounding Boxes) the key objects mentioned during the reasoning process. This significantly enhances the interpretability and precision of multimodal data.

Unlike traditional methods, this pipeline uses a **Single VLM (e.g., Qwen2.5-VL)** to handle both "Reasoning" and "Grounding" tasks, making the process streamlined and efficient.

We support the following application scenarios:

* **Enhanced Multimodal Data Construction**: Adding interpretability and grounding annotations to VQA datasets.
* **Complex Scene Understanding**: Generating detailed reasoning steps containing object coordinates.
* **Model Reasoning Training**: Building data to train models to be "grounded" and reduce hallucinations.

The main process of the pipeline includes:

1. **CoT Generation**: The model generates step-by-step reasoning text and extracts key nouns.
2. **Keyword Parsing**: Cleaning and extracting keywords to be grounded from the generated text.
3. **Visual Grounding**: The model generates bounding boxes (BBoxes) for the extracted keywords.
4. **Information Injection**: Injecting BBox coordinates back into the reasoning text to form the final GCoT.

---

## 2. Quick Start

### Step 1: Create a Working Directory

```bash
mkdir run_gcot
cd run_gcot

```

### Step 2: Prepare the Script

Save the code in the "Pipeline Example" section below as `image_gcot_pipeline.py`.

### Step 3: Download Example Data

```bash
huggingface-cli download --repo-type dataset OpenDCAI/dataflow-demo-image --local-dir example_data

```

### Step 4: Run

```bash
python image_gcot_pipeline.py \
  --model_path "/path/to/Qwen2.5-VL-3B-Instruct" \
  --input_file "data/image_qa.jsonl"

```

---

## 3. Data Flow & Logic

### 1. **Input Data**

The input data for this process typically consists of standard VQA data:

* **image**: Path to the image file.
* **question**: Question about the image.
* **answer**: Standard answer to the question (used to assist CoT generation).

**Input Data Example**:

```json
{
    "image": "./images/cat_dog.jpg",
    "question": "Is the cat looking at the dog?",
    "answer": "Yes"
}

```

### 2. **Core Operator Logic**

This pipeline combines multiple fine-grained operators to achieve complex GCoT generation logic:

#### A. **CoT Generation (PromptTemplatedVQAGenerator)**

Uses a predefined `GCOT_PROMPT_TEMPLATE` to guide the model to generate "Step-by-step Reasoning" and a "Keyword List".

* **Prompt Strategy**: Asks the model to output in the format `Step 1: ...`, `Step 2: ...`, `Keywords: ...`.
* **Output**: Raw string containing reasoning text and keywords.

#### B. **Text Cleaning & Extraction (FunctionalRefiner)**

Uses custom functions to parse the output from the previous step:

* `extract_clean_cot_logic`: Strips the keyword section, keeping pure CoT text.
* `extract_keywords_logic`: Parses the content after `Keywords:` to generate a Python List.

#### C. **Visual Grounding (VLMBBoxGenerator)**

Calls the VLM's grounding capability to generate bounding boxes for each extracted keyword.

* **Input**: Image + List of Keywords.
* **Output**: Dictionary mapping keywords to bounding box coordinates.

#### D. **Coordinate Injection (FunctionalRefiner)**

Uses the `inject_bboxes_logic` function to intelligently insert the generated BBox coordinates back into the original CoT text after the corresponding words.

### 3. **Output Data**

Finally, the output data generated by the pipeline will contain the following key fields:

* **raw_cot_output**: Raw text generated by the model.
* **cleaned_cot**: Cleaned reasoning text.
* **bbox_mapping**: Mapping of keywords to their coordinates.
* **gcot**: Final result, reasoning chain containing coordinate information.

**Output Data Example (gcot field)**:

```text
Step 1: Locate the cat [200, 300, 400, 500]. The cat is sitting on the left.
Step 2: Locate the dog [500, 300, 700, 500]. The dog is sleeping on the right.
Step 3: Observe their gaze. The cat is facing the dog.
Answer: Yes

```

---

## 4. Pipeline Example

Below is the complete `ImageGCoTPipeline` code implementation.

```python
import re
from typing import List, Dict, Any
import argparse
import torch
from dataflow.utils.storage import FileStorage
from dataflow.serving.local_model_vlm_serving import LocalModelVLMServing_vllm

from dataflow.operators.core_vision import PromptTemplatedVQAGenerator, VLMBBoxGenerator
from dataflow.operators.core_text import FunctionalRefiner
from dataflow.prompts.prompt_template import NamedPlaceholderPromptTemplate

# 定义 Prompt 模板，强制模型输出推理步骤和关键词
GCOT_PROMPT_TEMPLATE = (
    "Question: {question}\n"
    "Answer: {answer}\n\n"
    "Task: Provide a detailed step-by-step reasoning (Chain-of-Thought) that explains "
    "how to arrive at this answer based on the image.\n"
    "Then, extract key nouns and objects mentioned in your reasoning that are "
    "visible in the image and can be spatially located.\n\n"
    "Format:\n"
    "Step 1: ...\n"
    "Step 2: ...\n"
    "Answer: {answer}\n"
    "Keywords: object1, object2\n"
)

DEFAULT_BBOX_PROMPT = 'Detect "{keyword}".'

# ----------------- 辅助逻辑函数 ----------------- #

def _parse_base(text: str) -> Dict[str, Any]:
    """基础解析逻辑：分离 CoT 文本和 Keywords 行"""
    if not text: return {"cot": "", "keywords": []}
    lines = text.split('\n')
    cot_lines = []
    keywords = []
    for line in lines:
        if line.strip().lower().startswith('keywords:'):
            keyword_str = line.split(':', 1)[-1].strip()
            # 简单的分词处理
            raw_kws = [kw.strip().strip('.,;:!?"\'') for kw in keyword_str.replace(';', ',').split(',')]
            keywords = [k for k in raw_kws if k]
        else:
            cot_lines.append(line)
    return {"cot": '\n'.join(cot_lines).strip(), "keywords": keywords}

def extract_clean_cot_logic(text: str) -> str:
    return _parse_base(text)["cot"]

def extract_keywords_logic(text: str) -> List[str]:
    return _parse_base(text)["keywords"]

def inject_bboxes_logic(cot_text: str, bbox_map: Dict[str, List[str]]) -> str:
    """将 BBox 注入回 CoT 文本"""
    if not cot_text or not bbox_map: return cot_text
    # 优先匹配长词，避免子串误匹配
    sorted_keywords = sorted(bbox_map.keys(), key=lambda x: len(x), reverse=True)
    result_text = cot_text
    replaced = set()
    
    for keyword in sorted_keywords:
        if keyword in replaced: continue
        # 简单策略：只在 'Answer:' 之前注入，防止破坏答案区
        answer_pos = result_text.find('Answer:')
        search_limit = answer_pos if answer_pos != -1 else len(result_text)
        
        # 大小写不敏感查找
        pos = result_text.lower().find(keyword.lower(), 0, search_limit)
        if pos == -1: continue
        
        boxes = bbox_map[keyword] # List[str]
        box_str = "".join(boxes)
        # 替换：保留原词，追加 Box
        replacement = f"{keyword} {box_str}"
        
        result_text = result_text[:pos] + replacement + result_text[pos + len(keyword):]
        replaced.add(keyword)
    return result_text

# ----------------- 流水线定义 ----------------- #

class ImageGCoTPipeline:
    def __init__(
        self,
        model_path: str,
        *,
        first_entry_file: str,
        cache_path: str = "./cache_gcot",
        file_name_prefix: str = "gcot",
        # Keys 配置
        question_key: str = "question",
        answer_key: str = "answer",
        image_key: str = "image",
        output_key: str = "gcot",
        vllm_max_tokens: int = 512
    ):
        # 1. 存储初始化
        self.storage = FileStorage(
            first_entry_file_name=first_entry_file,
            cache_path=cache_path,
            file_name_prefix=file_name_prefix,
            cache_type="jsonl"
        )
        
        # 2. 模型服务 (单一模型)
        self.vlm_serving = LocalModelVLMServing_vllm(
            hf_model_name_or_path=model_path,
            vllm_tensor_parallel_size=1,
            vllm_temperature=0.7,
            vllm_max_tokens=vllm_max_tokens
        )
        
        self.keys = {
            "q": question_key,
            "a": answer_key,
            "img": image_key,
            "raw_cot": "raw_cot_output",
            "clean_cot": "cleaned_cot",
            "keywords": "extracted_keywords",
            "bbox_map": "bbox_mapping",
            "final": output_key
        }

        # 3. 算子链配置
        
        # Step A: 生成 CoT 和 Keywords
        self.op_gen_cot = PromptTemplatedVQAGenerator(
            serving=self.vlm_serving,
            system_prompt="You are a helpful assistant.",
            prompt_template=NamedPlaceholderPromptTemplate(template=GCOT_PROMPT_TEMPLATE)
        )
        
        # Step B: 解析清洗 CoT
        self.op_extract_cot = FunctionalRefiner(func=extract_clean_cot_logic)
        
        # Step C: 解析 Keywords
        self.op_extract_kws = FunctionalRefiner(func=extract_keywords_logic)

        # Step D: 生成 BBox (Grounding)
        self.op_bbox_gen = VLMBBoxGenerator(
            serving=self.vlm_serving,
            prompt_template=DEFAULT_BBOX_PROMPT
        )
        
        # Step E: 注入 BBox 到 CoT
        self.op_inject = FunctionalRefiner(func=inject_bboxes_logic)

    def forward(self):
        print(">>> [Pipeline] Step 1: Generating CoT...")
        self.op_gen_cot.run(
            self.storage.step(),
            input_image_key=self.keys["img"],
            output_answer_key=self.keys["raw_cot"],
            question=self.keys["q"],
            answer=self.keys["a"]
        )
        
        print(">>> [Pipeline] Step 2: Parsing Outputs...")
        self.op_extract_cot.run(
            self.storage.step(),
            output_key=self.keys["clean_cot"],
            text=self.keys["raw_cot"]
        )
        self.op_extract_kws.run(
            self.storage.step(),
            output_key=self.keys["keywords"],
            text=self.keys["raw_cot"]
        )
        
        print(">>> [Pipeline] Step 3: Generating BBoxes (Grounding)...")
        self.op_bbox_gen.run(
            self.storage.step(),
            input_image_key=self.keys["img"],
            input_kws_key=self.keys["keywords"],
            output_key=self.keys["bbox_map"]
        )
        
        print(">>> [Pipeline] Step 4: Injecting GCoT...")
        self.op_inject.run(
            self.storage.step(),
            output_key=self.keys["final"],
            cot_text=self.keys["clean_cot"],
            bbox_map=self.keys["bbox_map"]
        )
        
        print(f">>> [Pipeline] Done. Final GCoT saved to: {self.keys['final']}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_file", default="dataflow/example/image_to_text_pipeline/image_qa_result.jsonl")
    parser.add_argument("--model_path", default="Qwen/Qwen2.5-VL-3B-Instruct")
    
    args = parser.parse_args()
    
    pipe = ImageGCoTPipeline(
        model_path=args.model_path,
        first_entry_file=args.input_file
    )
    pipe.forward()

```
